{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ganaya-nih/NHLBI_Workshop_BigData092023/blob/main/AllofUs_SmokingPheWAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xaiSv5LYzG3J"
      },
      "outputs": [],
      "source": [
        "import multiprocessing\n",
        "multiprocessing.cpu_count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzBp4uDwzG3X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as mpl\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import math\n",
        "from IPython.display import display, HTML\n",
        "from datetime import date\n",
        "import multiprocessing\n",
        "\n",
        "import os\n",
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0KxMz_hjzG3Z"
      },
      "outputs": [],
      "source": [
        "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
        "my_bucket"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePzx2UiSzG3a"
      },
      "source": [
        "__Reading Phecode files__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hqwHw-AzzG3d"
      },
      "outputs": [],
      "source": [
        "rollup_map=pd.read_csv(\"phecode_rollup_map.csv\")\n",
        "ICD9_Phecodes=pd.read_csv(\"ICDPhecodes.csv\")\n",
        "phecodes=pd.read_csv(\"ICDPhecodes.csv\")\n",
        "ICD9_exclude=pd.read_csv(\"ICD9PhecodeExclude.csv\")\n",
        "ICD9_IC10_Phecodes=pd.read_csv(\"phecode_map_icd9_10.csv\")\n",
        "# strip off extra column\n",
        "ICD9_IC10_Phecodes = ICD9_IC10_Phecodes.iloc[:, 1:]\n",
        "phecode_info=pd.read_csv(\"pheinfo.csv\")\n",
        "sex_at_birth_restriction = pd.read_csv(\"sex_at_birth_restriction[40].csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Ci2N2fJzG3f"
      },
      "source": [
        "__<font color='red'>Warning</font>__\n",
        "- Individual-level records data CANNOT be downloaded ( monitoring data transfer)\n",
        "- In publications: results with numbers less than 20 should be suppressed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Iyr6hjFzG3h"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cRN2MvOWzG3i"
      },
      "outputs": [],
      "source": [
        "phecodes_list=ICD9_IC10_Phecodes[\"phecode\"].unique().tolist()\n",
        "CDR_version=os.getenv(\"WORKSPACE_CDR\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53BJB7wezG3j"
      },
      "outputs": [],
      "source": [
        "# Need to see which codes are in observation\n",
        "query = (\"\"\"\n",
        "SELECT distinct observation_source_value, vocabulary_id\n",
        "FROM\n",
        "    (SELECT DISTINCT person_id, observation_source_concept_id, observation_source_value, observation_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".observation`) AS obs\n",
        "     INNER JOIN\n",
        "        (SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "            where (vocabulary_id ='ICD9CM') or\n",
        "            (vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = obs.observation_source_concept_id\n",
        "\"\"\")\n",
        "observation_codes = pd.read_gbq(query, dialect=\"standard\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DnpfVyN5zG3k"
      },
      "outputs": [],
      "source": [
        "observation_icd9 = observation_codes[observation_codes.vocabulary_id==\"ICD9CM\"][\"observation_source_value\"]\n",
        "observation_icd10 = observation_codes[observation_codes.vocabulary_id==\"ICD10CM\"][\"observation_source_value\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SsaQsrCzG3l"
      },
      "source": [
        "__Following function extracts the Phecodes from two EHR tables: Condition_occurrence and observation__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tn84vHKrzG3m"
      },
      "outputs": [],
      "source": [
        "def getPhecodeParticipants(phecodes_batch, return_dict, phecodes_list, CDR_version, num_processes):\n",
        "    \"\"\"\n",
        "    Batching function for parallel extraction of participant Phecodes\n",
        "    ======================================================================================================\n",
        "    phecodes_batch: Pandas Dataframe of Phecodes\n",
        "    return_dict:\n",
        "    phecodes_list: List of phecodes to process\n",
        "    CDR_version: String of current cdr version\n",
        "    ## need to include rollup\n",
        "    \"\"\"\n",
        "    size = int(np.ceil(len(phecodes_list)/num_processes))\n",
        "    phecodes_code_list=phecodes_list[phecodes_batch*size:(phecodes_batch+1)*size]\n",
        "    # changing for 4 cores\n",
        "    #phecodes_code_list=phecodes_list[phecodes_batch*235:(phecodes_batch+1)*235]\n",
        "    # for particular codes, there are rollups, so we need to count on the fly\n",
        "    # note, there are icd9 codes that live in observation:\n",
        "    phecodes=ICD9_IC10_Phecodes[ICD9_IC10_Phecodes[\"phecode\"].isin(phecodes_code_list)]\n",
        "\n",
        "    ## ICD Codes in condition_occurrence (if a code is in observation, then it shouldn't query anything here)\n",
        "    icd9_codes_cond=phecodes[phecodes['vocabulary_id']=='ICD9CM'][\"code\"].tolist()\n",
        "    #\n",
        "    icd9_codes_cond_str=\"'\"+\"','\".join(icd9_codes_cond)+\"'\"\n",
        "\n",
        "    icd10_codes_cond=phecodes[phecodes['vocabulary_id']=='ICD10CM'][\"code\"].tolist()\n",
        "    icd10_codes_cond_str=\"'\"+\"','\".join(icd10_codes_cond)+\"'\"\n",
        "\n",
        "    ## ICD Codes in observation\n",
        "\n",
        "    icd9_codes_obs=np.intersect1d(observation_icd9,phecodes[phecodes['vocabulary_id']=='ICD9CM'][\"code\"].tolist()).tolist()\n",
        "    #\n",
        "    icd9_codes_obs_str=\"'\"+\"','\".join(icd9_codes_obs)+\"'\"\n",
        "\n",
        "    icd10_codes_obs=np.intersect1d(observation_icd10,phecodes[phecodes['vocabulary_id']=='ICD10CM'][\"code\"].tolist()).tolist()\n",
        "    icd10_codes_obs_str=\"'\"+\"','\".join(icd10_codes_obs)+\"'\"\n",
        "        # there's a subtlety here that we need icd10 cm\n",
        "    query=\"\"\"SELECT DISTINCT icd.person_id,condition_start_date as start_date,condition_concept_id as cid,concept_code,vocabulary_id FROM `\"\"\"+CDR_version+\"\"\".concept`\n",
        "    c  INNER JOIN `\"\"\"+CDR_version+\"\"\".condition_occurrence` icd  ON icd.condition_source_concept_id=c.concept_id\n",
        "    WHERE vocabulary_id ='ICD9CM' AND  concept_code IN (\"\"\"+icd9_codes_cond_str+\"\"\")\n",
        "    ORDER BY condition_start_date\"\"\"\n",
        "    icdcodes1_cond=pd.read_gbq(query, dialect=\"standard\")\n",
        "    query=\"\"\"SELECT DISTINCT icd.person_id,condition_start_date as start_date,condition_concept_id as cid,concept_code, vocabulary_id FROM `\"\"\"+CDR_version+\"\"\".concept`\n",
        "    c  INNER JOIN `\"\"\"+CDR_version+\"\"\".condition_occurrence` icd  ON c.concept_id = icd.condition_source_concept_id\n",
        "    WHERE vocabulary_id ='ICD10CM' AND  concept_code IN (\"\"\"+icd10_codes_cond_str+\"\"\")\n",
        "    ORDER BY condition_start_date\"\"\"\n",
        "    icdcodes2_cond=pd.read_gbq(query, dialect=\"standard\")\n",
        "\n",
        "    #Now observations\n",
        "\n",
        "    query=\"\"\"SELECT DISTINCT icd.person_id,observation_date as start_date, observation_concept_id as cid,concept_code,vocabulary_id FROM `\"\"\"+CDR_version+\"\"\".concept`\n",
        "    c  INNER JOIN `\"\"\"+CDR_version+\"\"\".observation` icd  ON icd.observation_source_concept_id=c.concept_id\n",
        "    WHERE vocabulary_id ='ICD9CM' AND  concept_code IN (\"\"\"+icd9_codes_obs_str+\"\"\")\n",
        "    ORDER BY start_date\"\"\"\n",
        "    icdcodes1_obs=pd.read_gbq(query, dialect=\"standard\")\n",
        "    query=\"\"\"SELECT DISTINCT icd.person_id,observation_date as start_date,observation_concept_id as cid, concept_code, vocabulary_id FROM `\"\"\"+CDR_version+\"\"\".concept`\n",
        "    c  INNER JOIN `\"\"\"+CDR_version+\"\"\".observation` icd  ON c.concept_id = icd.observation_source_concept_id\n",
        "    WHERE vocabulary_id ='ICD10CM' AND  concept_code IN (\"\"\"+icd10_codes_obs_str+\"\"\")\n",
        "    ORDER BY start_date\"\"\"\n",
        "\n",
        "    icdcodes2_obs=pd.read_gbq(query, dialect=\"standard\")\n",
        "\n",
        "    icdcodes=pd.concat([icdcodes1_cond,icdcodes2_cond,icdcodes1_obs,icdcodes2_obs]).drop_duplicates() # drop duplicates within vocab, person id and date\n",
        "\n",
        "    patients_phcode_count=icdcodes[[\"person_id\",\"start_date\",\"concept_code\",\"vocabulary_id\"]].drop_duplicates()[[\"person_id\",\"concept_code\",\"start_date\",\"vocabulary_id\"]]\n",
        "\n",
        "    patients_phcode_count=pd.merge(phecodes[[\"code\",\"phecode\",\"vocabulary_id\"]],patients_phcode_count,left_on=[\"code\", \"vocabulary_id\"],right_on=[\"concept_code\", \"vocabulary_id\"])\n",
        "\n",
        "    return_dict[phecodes_batch]=patients_phcode_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pV91pKSzG3o"
      },
      "source": [
        "__The following code extract the count of Phecodes occurrence in participants records__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ly9Jz6YvzG3p"
      },
      "outputs": [],
      "source": [
        "def phecode_counts(num_processes, phecodes_list, CDR_version):\n",
        "    size = int(np.ceil(len(phecodes_list)/num_processes))\n",
        "    print(\"Processing phecodes in blocks of size: \" + str(size))\n",
        "\n",
        "    manager = multiprocessing.Manager()\n",
        "    return_dict = manager.dict()\n",
        "    jobs = []\n",
        "    for i in range(num_processes):\n",
        "        p = multiprocessing.Process(target=getPhecodeParticipants, args=(i,return_dict, phecodes_list, CDR_version, num_processes)) #phecodes list needs to be included\n",
        "        jobs.append(p)\n",
        "        p.start()\n",
        "    # close out and join jobs\n",
        "    for proc in jobs:\n",
        "        proc.join()\n",
        "    # make into a dataframe\n",
        "    phecodes_patients_list=pd.concat(list(return_dict.values()))\n",
        "\n",
        "    # now merge to rollup map\n",
        "    # Adjust to including the observation table\n",
        "\n",
        "    phecodes_patients_list_merge = pd.merge(rollup_map, phecodes_patients_list,left_on = 'code', right_on = 'phecode')[[\"person_id\",\"phecode_unrolled\",\"start_date\"]].drop_duplicates()\n",
        "    ## clean a little bit more\n",
        "    # get rid of any additional\n",
        "    # Including the observation table\n",
        "    phecodes_patients_counts_tmp=phecodes_patients_list_merge[[\"person_id\",\"phecode_unrolled\",\"start_date\"]].groupby([\"person_id\",\"phecode_unrolled\"],as_index=False).count()\n",
        "    #\n",
        "    ## so as not to break the other code\n",
        "    phecodes_patients_counts_tmp.columns = [\"person_id\", \"phecode\", 'count']\n",
        "\n",
        "    return(phecodes_patients_counts_tmp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vo0KOmljzG3q"
      },
      "outputs": [],
      "source": [
        "phecodes_patients_counts = phecode_counts(num_processes=36, phecodes_list=phecodes_list, CDR_version = CDR_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VN6vnnEzG3r"
      },
      "source": [
        "__To save some credit and time, make sure to save the file__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JgrF7CR0zG3r"
      },
      "outputs": [],
      "source": [
        "phecodes_patients_counts.to_csv(\"phecodes_patients_counts.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FHJneXmbzG3r"
      },
      "source": [
        "__To run the PheWAS, we need covriaites that we will adjust to in the PheWAS model such as race, sex at birth, EHR length, etc__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YX1MUoxQzG3s"
      },
      "outputs": [],
      "source": [
        "def make_covariates(df_indep_var,CDR_version):\n",
        "    # first query from condition occurrence to get number of distinct codes and ehr length\n",
        "    query=\"\"\"SELECT person_id, min(condition_start_date) as min_cond_date,max(condition_start_date)as max_cond_date,COUNT(DISTINCT condition_concept_id) as cond_code_cnt\n",
        "    FROM `\"\"\"+CDR_version+\"\"\".condition_occurrence` WHERE condition_start_date >='1980-01-01'\n",
        "    GROUP BY person_id \"\"\"\n",
        "\n",
        "    ehr_covariate=pd.read_gbq(query, dialect=\"standard\")\n",
        "    print(ehr_covariate.shape[0],ehr_covariate[pd.isna(ehr_covariate[\"min_cond_date\"])==True].shape[0],ehr_covariate[pd.isna(ehr_covariate[\"max_cond_date\"])==True].shape[0] )\n",
        "\n",
        "    # EHR Length is a bit subtle here. Observation date is a little tricky because birthdate is used in observation.\n",
        "    # What we need to do is take the relevant codes from observation and find the minimum and maximum dates overall\n",
        "    query = (\"\"\"\n",
        "        SELECT distinct person_id, min(observation_date) as min_obs_date, max(observation_date) as max_obs_date, COUNT(DISTINCT observation_concept_id) as observation_code_cnt\n",
        "        FROM\n",
        "            (SELECT DISTINCT person_id, observation_source_concept_id, observation_source_value, observation_date, observation_concept_id\n",
        "                FROM `\"\"\"+ str(CDR_version) +\"\"\".observation`) AS obs\n",
        "             INNER JOIN\n",
        "                (SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "                    FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "                    where (vocabulary_id ='ICD9CM') or\n",
        "                    (vocabulary_id ='ICD10CM')) as concept\n",
        "                    on concept.concept_id = obs.observation_source_concept_id\n",
        "        group by person_id\n",
        "        \"\"\")\n",
        "    observation_code_dates = pd.read_gbq(query, dialect=\"standard\")\n",
        "    print(observation_code_dates.shape[0],observation_code_dates[pd.isna(observation_code_dates[\"min_obs_date\"])==True].shape[0],observation_code_dates[pd.isna(observation_code_dates[\"max_obs_date\"])==True].shape[0] )\n",
        "\n",
        "    # note, this procedure takes into consideration NaTs, so if a person didn't have an observation code, it will\n",
        "    # automatically take the minimum condition occurrence date.\n",
        "    ehr_covariate_plus = pd.merge(observation_code_dates, ehr_covariate, on=\"person_id\", how = 'outer')\n",
        "    ehr_covariate_plus[\"min_obs_date\"]=pd.to_datetime(ehr_covariate_plus[\"min_obs_date\"])\n",
        "    ehr_covariate_plus[\"min_cond_date\"]=pd.to_datetime(ehr_covariate_plus[\"min_cond_date\"])\n",
        "    ehr_covariate_plus[\"max_obs_date\"]=pd.to_datetime(ehr_covariate_plus[\"max_obs_date\"])\n",
        "    ehr_covariate_plus[\"max_cond_date\"]=pd.to_datetime(ehr_covariate_plus[\"max_cond_date\"])\n",
        "    ehr_covariate_plus[\"min_date\"] = ehr_covariate_plus[[\"min_obs_date\",\"min_cond_date\"]].min(axis =1,skipna=True)\n",
        "    ehr_covariate_plus[\"max_date\"] = ehr_covariate_plus[[\"max_obs_date\",\"max_cond_date\"]].max(axis =1,skipna=True)\n",
        "    ehr_covariate_plus[\"code_cnt\"] =  ehr_covariate_plus[[\"cond_code_cnt\",\"observation_code_cnt\"]].sum(axis =1)\n",
        "    # for efficiency sake drop the extra columns\n",
        "    ehr_covariate_plus = ehr_covariate_plus.drop(columns = [\"min_obs_date\",\"min_cond_date\",\"max_obs_date\",\"max_cond_date\",\"cond_code_cnt\",\"observation_code_cnt\"])\n",
        "    # ehr length defined as time between minimum of relevant observation dates\n",
        "    print(ehr_covariate_plus['max_date']-ehr_covariate_plus['min_date'])\n",
        "    print(ehr_covariate_plus.shape[0],ehr_covariate_plus[pd.isna(ehr_covariate_plus[\"min_date\"])==True].shape[0],ehr_covariate_plus[pd.isna(ehr_covariate_plus[\"max_date\"])==True].shape[0] )\n",
        "\n",
        "    ehr_covariate_plus[\"ehr_length\"]=(ehr_covariate_plus['max_date']-ehr_covariate_plus['min_date']).apply(lambda x:x.days)\n",
        "\n",
        "    # Now Demographics\n",
        "\n",
        "    query=\"\"\"SELECT DISTINCT p.person_id,sex_at_birth_concept_id,gender_concept_id,race_concept_id,ethnicity_concept_id,year_of_birth, month_of_birth, day_of_birth FROM\n",
        "    `\"\"\"+CDR_version+\"\"\".person` p\n",
        "    \"\"\"\n",
        "    demo_patients=pd.read_gbq(query, dialect=\"standard\")\n",
        "    demo_patients[\"birthdate\"] =pd.to_datetime(demo_patients[\"year_of_birth\"].apply(str)+\n",
        "                                               \"-\"+demo_patients[\"month_of_birth\"].apply(str)+\n",
        "                                               \"-\"+demo_patients[\"day_of_birth\"].apply(str))\n",
        "    demo_patients[\"age_today\"] = (datetime.today()-demo_patients[\"birthdate\"])/np.timedelta64(1,'Y')\n",
        "    demo_patients=demo_patients[demo_patients[\"person_id\"].isin(phecodes_patients_counts[\"person_id\"])]\n",
        "\n",
        "    # tidy up\n",
        "    demo_patients_merge = demo_patients\n",
        "    #demo_patients_merge=pd.merge(demo_patients, race_generalized,on=\"person_id\")\n",
        "    demo_patients_merge=pd.merge(demo_patients_merge, ehr_covariate_plus, on=\"person_id\") # now merge to other ehr covariates # amended 04/17/2020\n",
        "    demo_patients_merge[\"age_at_last_event\"] = (demo_patients_merge[\"max_date\"]-demo_patients_merge[\"birthdate\"])/np.timedelta64(1,'Y')\n",
        "\n",
        "    #############\n",
        "    #race\n",
        "    #############\n",
        "    white = 8527\n",
        "    black_aa = 8516\n",
        "    asian = 8515\n",
        "    # other\n",
        "    more_than_one = 2000000008\n",
        "    none_of_these = 45882607\n",
        "    another_single_pop=2000000001\n",
        "    other = [more_than_one, none_of_these,another_single_pop]\n",
        "    #Unknown\n",
        "    prefer_not_answer = 1177221\n",
        "    skip = 903096\n",
        "    no_match = 0\n",
        "    unknown_race = [prefer_not_answer, skip, no_match]\n",
        "    ## update indicators\n",
        "    demo_patients_merge[\"white\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"AF\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"Asian\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"race_unk\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"Other\"]=[0]*demo_patients_merge.shape[0]\n",
        "\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"race_concept_id\"]==white,\"white\"]=1\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"race_concept_id\"]==black_aa,\"AF\"]=1\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"race_concept_id\"]==asian,\"Asian\"]=1\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"race_concept_id\"].isin(unknown_race),\"race_unk\"]=1\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"race_concept_id\"].isin(other),\"Other\"]=1\n",
        "\n",
        "\n",
        "    #############\n",
        "    #ethnicity\n",
        "    #############\n",
        "    #hisp/latino\n",
        "    hisp_latino = 38003563\n",
        "    # not hisp/latino\n",
        "    not_hisp_lat = 38003564\n",
        "    none_of_these = 45882607\n",
        "    not_hisp = [not_hisp_lat, none_of_these]\n",
        "    # unknown\n",
        "    prefer_no_ans = 1177221\n",
        "    skip= 903096\n",
        "    unknown_eth = [prefer_no_ans, skip]\n",
        "    # update indicators\n",
        "\n",
        "    demo_patients_merge[\"hisp_lat\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"not_hisp_lat\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"unk_eth\"]=[0]*demo_patients_merge.shape[0]\n",
        "    # sex at birth\n",
        "    demo_patients_merge.loc[ demo_patients_merge[\"ethnicity_concept_id\"]==hisp_latino,\"hisp_lat\"]=1\n",
        "    demo_patients_merge.loc[ demo_patients_merge[\"ethnicity_concept_id\"]==not_hisp_lat,\"not_hisp_lat\"]=1\n",
        "    demo_patients_merge.loc[(demo_patients_merge[\"ethnicity_concept_id\"].isin(unknown_eth)),\"unk_eth\"]=1\n",
        "\n",
        "\n",
        "    #############\n",
        "    #sex at birth\n",
        "    #############\n",
        "    female = 45878463\n",
        "    male = 45880669\n",
        "    unknown = 2000000009 # prefer not to answer\n",
        "    zer = 0\n",
        "    ## update indicators\n",
        "    demo_patients_merge[\"female\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"male\"]=[0]*demo_patients_merge.shape[0]\n",
        "    demo_patients_merge[\"unk_sex\"]=[0]*demo_patients_merge.shape[0]\n",
        "    # sex at birth\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"sex_at_birth_concept_id\"]==female,\"female\"]=1\n",
        "    demo_patients_merge.loc[demo_patients_merge[\"sex_at_birth_concept_id\"]==male,\"male\"]=1\n",
        "    demo_patients_merge.loc[(demo_patients_merge[\"sex_at_birth_concept_id\"]==unknown)|\n",
        "                            (demo_patients_merge[\"sex_at_birth_concept_id\"]==zer),\"unk_sex\"]=1\n",
        "    # now merge to the indep var of interest\n",
        "    demo_patients_cov=pd.merge(demo_patients_merge,df_indep_var, on = \"person_id\")\n",
        "    demo_patients_cov=demo_patients_cov.drop_duplicates()\n",
        "    return(demo_patients_cov)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7Uevg6QzG3t"
      },
      "source": [
        "__Defining PheWAS class that initializes variables, run Phewas and plot Manhattan plot__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88hpMhU3zG3u"
      },
      "outputs": [],
      "source": [
        "class PheWAS_Pool:\n",
        "    \"\"\"\n",
        "    Class for performing PheWAS\n",
        "    ======================================================================================================\n",
        "    phecode_counts: Pandas Dataframe of Phecodes\n",
        "    covariates: Pandas Dataframe of covariates to include in the analysis\n",
        "    indep_var: String indicating the column in covariates that is the independent variable of interest\n",
        "    CDR_version: String indicating CDR version\n",
        "    phecode_process: list for phecodes to process\n",
        "    min_cases: minimum number of cases for an individual phenotype to be analyzed\n",
        "    cores: if not \"\", then specify number of cores to use in the analysis\n",
        "    \"\"\"\n",
        "    def __init__(self, phecode_counts,\n",
        "                 covariates,\n",
        "                 indep_var_of_interest=\"\",\n",
        "                 CDR_version='R2019Q4R3',\n",
        "                 phecode_process = 'all',\n",
        "                 min_cases = 100,\n",
        "                 independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                        \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "                 genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\"unk_sex\",\"male\",\n",
        "                                                   \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "                 show_res = False,\n",
        "                 cores=\"\"):\n",
        "        print(\"~~~~~~~~~~~~~~~        Creating PheWAS AOU Object           ~~~~~~~~~~~~~~~~~~~~~\")\n",
        "        # create instance attributes\n",
        "        self.indep_var_of_interest = indep_var_of_interest\n",
        "        #update 09_5_2019: only process phecodes passed in phecode counts\n",
        "        if phecode_process =='all':\n",
        "            self.phecode_list = phecode_counts[\"phecode\"].unique().tolist()\n",
        "        else:\n",
        "            self.phecode_list = phecode_process\n",
        "        self.CDR_version = CDR_version\n",
        "        self.cores = cores\n",
        "        print(\"~~~~~~~~~~~~~~~       Merging Phecodes and Covariates       ~~~~~~~~~~~~~~~~~~~~~\")\n",
        "        self.demo_patients_phecodes = pd.merge(covariates,phecode_counts, on = \"person_id\")\n",
        "        self.show_res = show_res\n",
        "        self.independent_var_names = independent_var_names\n",
        "        self.independent_var_names= list(np.append(np.array([self.indep_var_of_interest]),self.independent_var_names))\n",
        "        self.genderspec_independent_var_names = genderspec_independent_var_names\n",
        "        self.genderspec_independent_var_names= list(np.append(np.array([self.indep_var_of_interest]),self.genderspec_independent_var_names))\n",
        "        self.remove_dup = list(np.append(np.array([\"person_id\"]),self.independent_var_names))\n",
        "        self.min_cases = min_cases\n",
        "\n",
        "    def runPheLogit(self, phecodes):\n",
        "        # diagnostics\n",
        "        for phecode in phecodes:\n",
        "            ## init error\n",
        "            error = \"Other Error\"\n",
        "            try:\n",
        "                # First, need to define the exclusions, sufficient to just use the ICD9 one since the overall phecodes\n",
        "                # are the same\n",
        "                phecode_exclusions=ICD9_exclude[ICD9_exclude[\"code\"]==phecode][\"exclusion_criteria\"].unique().tolist()\n",
        "\n",
        "                ## we need to do sex specific counting here.\n",
        "                ## First find all people with at least 2 of the phecode\n",
        "                cases=self.demo_patients_phecodes[(self.demo_patients_phecodes[\"phecode\"]==phecode)&\n",
        "                                                  (self.demo_patients_phecodes[\"count\"]>=2)]\n",
        "\n",
        "                ## now determine if there is a sex specific restriction\n",
        "                ## this is convoluted, but it is written this way to avoid storing another copy of all the data in memory\n",
        "                male_only = sex_at_birth_restriction[sex_at_birth_restriction.phecode.astype(float)==phecode][\"male_only\"]==True\n",
        "                female_only = sex_at_birth_restriction[sex_at_birth_restriction.phecode.astype(float)==phecode][\"female_only\"]==True\n",
        "\n",
        "                # now, if sex at birth specific, filter all the people to the proper sex and set the right covariates\n",
        "                if np.array(male_only)[0]==True :\n",
        "                    analysis_independent_var_names=self.genderspec_independent_var_names\n",
        "                    cases=cases[cases['male']==1]#restrict to males\n",
        "\n",
        "                # if female only, then restrict to people who are female == 1\n",
        "                elif np.array(female_only)[0]==True:\n",
        "\n",
        "                    analysis_independent_var_names=self.genderspec_independent_var_names\n",
        "                    cases=cases[cases['female']==1]#restrict to females\n",
        "\n",
        "                # otherwise there is no restriction, so we can use sex at birth in the analyses\n",
        "                else:\n",
        "                    analysis_independent_var_names=self.independent_var_names\n",
        "\n",
        "                # Now cases have been properly modified and so we remove any duplicates and restrict the analysis to just the regressors\n",
        "                cases=cases[self.remove_dup].drop_duplicates()[analysis_independent_var_names]\n",
        "\n",
        "                # Now test to see if we have enough cases\n",
        "                # This is written like this to avoid unnecessary compute for phecodes\n",
        "                # for which we don't have enough cases\n",
        "\n",
        "                if cases.shape[0]>=self.min_cases:\n",
        "                    # if it passes\n",
        "                    # create set all people that need to be excluded\n",
        "                    exclude=self.demo_patients_phecodes[(self.demo_patients_phecodes[\"phecode\"].isin(np.append(phecode_exclusions,phecode)))]#&(control[\"concept_code\"]<=2)]\n",
        "\n",
        "                    ## if sex specific, restrict analyses to just that sex at birth\n",
        "                    if np.array(male_only)[0]==True:\n",
        "                        control=self.demo_patients_phecodes[(self.demo_patients_phecodes.person_id.isin(exclude.person_id)==False)&\n",
        "                                                            self.demo_patients_phecodes.male==1] # pick off just the males\n",
        "\n",
        "                    # if female only, then restrict to people who are female == 1\n",
        "                    elif np.array(female_only)[0]==True:\n",
        "                        control=self.demo_patients_phecodes[(self.demo_patients_phecodes.person_id.isin(exclude.person_id)==False)&\n",
        "                                                            (self.demo_patients_phecodes.female==1)] # pick off just the males\n",
        "\n",
        "                    # otherwise there is no restriction, so we can use sex at birth in the analyses\n",
        "                    else:\n",
        "                        control=self.demo_patients_phecodes[self.demo_patients_phecodes.person_id.isin(exclude.person_id)==False]\n",
        "\n",
        "                    # Now controls have been properly modified and so we remove any duplicates and restrict the analysis to just the regressors\n",
        "                    control=control[self.remove_dup].drop_duplicates()[analysis_independent_var_names]\n",
        "                    ############################################################################################\n",
        "                    ## Perform Logistic regression\n",
        "                    ## Now run through the logit function from stats models\n",
        "                    ############################################################################################\n",
        "                    y=[1]*cases.shape[0]+[0]*control.shape[0]\n",
        "                    regressors=pd.concat([cases,control])\n",
        "                    regressors=sm.tools.add_constant(regressors)\n",
        "                    logit = sm.Logit(y, regressors, missing = 'drop')\n",
        "                    result = logit.fit(disp=False)\n",
        "\n",
        "                    # choose to see results on the fly\n",
        "                    if self.show_res == True:\n",
        "                        print(result.summary())\n",
        "                    else:\n",
        "                        pass\n",
        "                    # return\n",
        "                    results_as_html = result.summary().tables[0].as_html()\n",
        "                    converged = pd.read_html(results_as_html)[0].iloc[5,1]\n",
        "                    # now other quants\n",
        "                    results_as_html = result.summary().tables[1].as_html()\n",
        "                    res = pd.read_html(results_as_html, header = 0, index_col=0)[0]\n",
        "                    p_value =  result.pvalues[self.indep_var_of_interest]\n",
        "                    beta_ind = result.params[self.indep_var_of_interest]\n",
        "                    conf_int_1=res.loc[self.indep_var_of_interest]['[0.025']\n",
        "                    conf_int_2=res.loc[self.indep_var_of_interest]['0.975]']\n",
        "\n",
        "                    self.return_dict[phecode] =[phecode,cases.shape[0],control.shape[0],p_value,\n",
        "                                                beta_ind, conf_int_1, conf_int_2, converged]\n",
        "                else:\n",
        "                    error = \"Error in Phecode: \"+str(phecode)+ \": Number of cases less than minimum of \"+str(self.min_cases)\n",
        "                del [control, cases, regressors]\n",
        "            except:\n",
        "                print(error)\n",
        "    # now define function for running the phewas\n",
        "    def run(self):\n",
        "        # we will use multiprocessing tool for this\n",
        "        #update_09_04_2019\n",
        "        manager = multiprocessing.Manager()\n",
        "        self.return_dict = manager.dict()\n",
        "        ## this needs to have a particular list structure for the multiprocessing\n",
        "        partitions = [list(ind) for ind in np.array_split(self.phecode_list, self.cores)] # just check for benchmark\n",
        "        pool = multiprocessing.Pool(processes=self.cores)\n",
        "        map_result = pool.map_async(self.runPheLogit, partitions) #what this is doing is iterating over the list of indices I made before\n",
        "        # and subsetting the phenotypes to just the batch in index i of indices\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "\n",
        "        logit_Phecode_results=[self.return_dict[k] for k in self.return_dict.keys()]\n",
        "        logit_Phecode_results=pd.DataFrame(logit_Phecode_results)\n",
        "        logit_Phecode_results.columns=[\"phecode\",\"cases\",\"control\",\"p_value\",\"beta_ind\",\"conf_int_1\", \"conf_int_2\", \"converged\"]\n",
        "\n",
        "        logit_Phecode_results[\"code_val\"] = logit_Phecode_results[\"phecode\"]\n",
        "        logit_Phecode_results[\"neg_p_log_10\"] = -np.log10(logit_Phecode_results[\"p_value\"])\n",
        "        logit_Phecode_results=pd.merge(phecode_info,logit_Phecode_results)\n",
        "        # now save logit phecode as attribute\n",
        "        self.logit_Phecode_results = logit_Phecode_results\n",
        "\n",
        "    def Manhattan_Plot_Plus(self, group = \"all\"):\n",
        "        \"\"\"\n",
        "        Method for plotting Manhattan Plot\n",
        "        ======================================================================================================\n",
        "        group: list of groups to display (e.g. neoplasms)\n",
        "        \"\"\"\n",
        "        PheWAS_results_ehr =self.logit_Phecode_results\n",
        "\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"darkorange1\",\"color\"]=\"orange\"\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"darkseagreen4\",\"color\"]=\"darkgreen\"\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"coral4\",\"color\"]=\"coral\"\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"chartreuse4\",\"color\"]=\"chartreuse\"\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"royalblue4\",\"color\"]=\"royalblue\"\n",
        "        PheWAS_results_ehr.loc[PheWAS_results_ehr[\"color\"]==\"gray50\",\"color\"]=\"gray\"\n",
        "        x2,y2 = 20,50\n",
        "        # subset to particular group\n",
        "\n",
        "        if group != \"all\":\n",
        "            PheWAS_results_ehr = PheWAS_results_ehr[PheWAS_results_ehr[\"group\"]==group]\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(15,8))\n",
        "        benf_corr=.05/phecodes.PheCode.unique().shape[0]\n",
        "        pos_beta=PheWAS_results_ehr[PheWAS_results_ehr[\"beta_ind\"]>=0]\n",
        "        neg_beta=PheWAS_results_ehr[PheWAS_results_ehr[\"beta_ind\"]<0]\n",
        "\n",
        "        ax.scatter(pos_beta[\"code_val\"], pos_beta[\"neg_p_log_10\"], c=pos_beta['color'],marker='^')\n",
        "        ax.scatter(neg_beta[\"code_val\"], neg_beta[\"neg_p_log_10\"], c=neg_beta['color'],marker='v')\n",
        "        ax.hlines(-np.log10(.05),0,PheWAS_results_ehr[\"code_val\"].max()+1,colors='r',label = \"0.05\")\n",
        "        ax.hlines(-np.log10(benf_corr),0,PheWAS_results_ehr[\"code_val\"].max()+1,colors='g',label = \"Bonferroni Threshold (0.05/1847)\")\n",
        "        PheWas_ticks=PheWAS_results_ehr[[\"phecode\",\"group\"]].groupby(\"group\",as_index=False).mean()\n",
        "\n",
        "        # reshape the final plot to just fit the phecodes in the subgroup\n",
        "        plt.xlim(PheWAS_results_ehr[\"phecode\"].min(),PheWAS_results_ehr[\"phecode\"].max())\n",
        "        plt.xticks(PheWas_ticks[\"phecode\"],PheWas_ticks[\"group\"],rotation=45,ha=\"right\")\n",
        "        pos_beta_top=pos_beta[pos_beta[\"p_value\"]<benf_corr].sort_values(\"neg_p_log_10\",ascending=False).iloc[:15,][[\"code_val\",\"neg_p_log_10\",\"description\"]]\n",
        "        #Drop infs\n",
        "        #\n",
        "        pos_beta_top = pos_beta_top[~np.isinf(pos_beta_top[\"neg_p_log_10\"])]\n",
        "        neg_beta_top=neg_beta[neg_beta[\"p_value\"]<benf_corr].sort_values(\"neg_p_log_10\",ascending=False).iloc[:10,][[\"code_val\",\"neg_p_log_10\",\"description\"]]\n",
        "        ## drop infs\n",
        "        neg_beta_top= neg_beta_top[~np.isinf(neg_beta_top[\"neg_p_log_10\"])]\n",
        "\n",
        "        for i,row in pos_beta_top.iterrows():\n",
        "            ax.annotate(row[\"description\"], (row[\"code_val\"], row[\"neg_p_log_10\"]),xytext=(x2, y2), textcoords='offset points',\n",
        "            arrowprops=dict(arrowstyle=\"->\", color='black'))\n",
        "        for i,row in neg_beta_top.iterrows():\n",
        "            ax.annotate(row[\"description\"], (row[\"code_val\"], row[\"neg_p_log_10\"]),xytext=(x2, y2), textcoords='offset points',\n",
        "            arrowprops=dict(arrowstyle=\"->\", color='black'))\n",
        "        # assign top pos and neg to self\n",
        "        self.pos_beta_top = pos_beta_top\n",
        "        self.neg_beta_top = neg_beta_top\n",
        "        from matplotlib.lines import Line2D\n",
        "        # add legend elements\n",
        "        legend_elements =  [Line2D([0], [0], color='g', lw=4, label='Bonferroni Correction'),\n",
        "                            Line2D([0], [0], color='r', lw=4, label='Nominal Significance Level'),\n",
        "                            Line2D([0], [0], marker='v', label='Protective Effect',\n",
        "                                  markerfacecolor='b', markersize=15),\n",
        "                           Line2D([0], [0], marker='^', label='Non-Protective Effect',\n",
        "                                  markerfacecolor='b', markersize=15),]\n",
        "        ax.legend(handles=legend_elements, bbox_to_anchor =(0.5,-0.27), loc='lower center')\n",
        "        ax.set_ylabel(r'$-\\log_{10}$(p-value)')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bR68AG9zG3w"
      },
      "source": [
        "__Extracting smoking status from EHR__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HptAkgjzG3w"
      },
      "outputs": [],
      "source": [
        "query = (\"\"\"\n",
        "SELECT distinct person_id,  count(distinct code_date) as count\n",
        "FROM\n",
        "    (SELECT DISTINCT person_id, code_date\n",
        "    from\n",
        "    (SELECT DISTINCT person_id, observation_source_concept_id, observation_source_value, observation_date as code_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".observation`) AS obs\n",
        "     INNER JOIN\n",
        "        (SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`x\n",
        "            where (concept_code = 'V15.82'\n",
        "            and vocabulary_id ='ICD9CM') or\n",
        "            (concept_code in('Z72.0','Z87.891','Z71.6')\n",
        "            and vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = obs.observation_source_concept_id\n",
        "    union all\n",
        "    (select DISTINCT person_id, code_date\n",
        "    from\n",
        "     (SELECT DISTINCT person_id, condition_source_concept_id, condition_source_value, condition_start_date as code_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".condition_occurrence`) AS cond\n",
        "     INNER JOIN\n",
        "        (\n",
        "        SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "            where (concept_code like '305.1%' or concept_code = 'V15.82' or concept_code like '649.0%'\n",
        "            or concept_code like \"989.84%\"\n",
        "            and vocabulary_id ='ICD9CM') or\n",
        "            (concept_code in('Z72.0') or concept_code like 'O99.33%' or concept_code like \"F17.2%\"\n",
        "                or concept_code like 'T65.2%'\n",
        "            and vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = cond.condition_source_concept_id\n",
        "            )  )\n",
        "group by person_id\n",
        "\"\"\")\n",
        "df_smoke_controls_exclusion = pd.read_gbq(query, dialect=\"standard\")\n",
        "df_smoke_controls_exclusion.to_csv(\"df_smoke_controls_exclusion.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwn4cj5bzG3x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8W0JARYzG3x"
      },
      "outputs": [],
      "source": [
        "query = (\"\"\"\n",
        "SELECT distinct person_id,  count(distinct code_date) as count\n",
        "FROM\n",
        "    (\n",
        "    (SELECT DISTINCT person_id, code_date\n",
        "    from\n",
        "    (SELECT DISTINCT person_id, observation_source_concept_id, observation_source_value, observation_date as code_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".observation`) AS obs\n",
        "     INNER JOIN\n",
        "        (SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "            where (concept_code = 'V15.82'\n",
        "            and vocabulary_id ='ICD9CM') or\n",
        "            (concept_code in('Z72.0','Z87.891','Z71.6')\n",
        "            and vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = obs.observation_source_concept_id)\n",
        "    union all\n",
        "    (select DISTINCT person_id, code_date\n",
        "    from\n",
        "     (SELECT DISTINCT person_id, condition_source_concept_id, condition_source_value, condition_start_date as code_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".condition_occurrence`) AS cond\n",
        "     INNER JOIN\n",
        "        (\n",
        "        SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "            where (concept_code like '305.1%' or concept_code = 'V15.82' or concept_code like '649.0%'\n",
        "            or concept_code like \"989.84%\"\n",
        "            and vocabulary_id ='ICD9CM') or\n",
        "            (concept_code in('Z72.0') or concept_code like 'O99.33%'\n",
        "                or (concept_code like 'F17.2%' and concept_code not like 'F17.22%')\n",
        "                or (concept_code like 'T65.2%' and concept_code not like 'T65.21%')\n",
        "            and vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = cond.condition_source_concept_id\n",
        "            )  )\n",
        "group by person_id\n",
        "\"\"\")\n",
        "df_smoke_cases_inclusion = pd.read_gbq(query, dialect=\"standard\")\n",
        "df_smoke_cases_inclusion.to_csv(\"df_smoke_cases_inclusion.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCiEYQTXzG3x"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOyZLrQOzG3y"
      },
      "outputs": [],
      "source": [
        "df_smoke_cases_inclusion[\"ever_smoke_ehr\"] = (df_smoke_cases_inclusion[\"count\"]>=2)+0\n",
        "# now -9 out the folks with one code\n",
        "df_smoke_cases_inclusion[\"ever_smoke_ehr\"] = df_smoke_cases_inclusion[\"ever_smoke_ehr\"].map({1:1,0:-9})\n",
        "df_smoke_cases_inclusion.ever_smoke_ehr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XD0HtSHuzG3y"
      },
      "outputs": [],
      "source": [
        "df_smoke_cases_inclusion.ever_smoke_ehr.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtngr3lUzG3y"
      },
      "outputs": [],
      "source": [
        "# now find everyone without any smoking\n",
        "# amend to include people with icd9cm and icd10 cm in observation\n",
        "# use df_smoke_controls_exclusion person_ids since we don't want anyone with chewing tobacco in controls\n",
        "query = (\"\"\"\n",
        "select distinct person_id from `\"\"\"+ str(CDR_version)+\"\"\".condition_occurrence`\n",
        "where person_id not in \"\"\"+\"(\"+' '.join([str(id)+\",\" for id in np.unique(df_smoke_controls_exclusion.person_id)])[:-1]+\")\"+\"\"\";\n",
        "\"\"\")\n",
        "smoke_nos = pd.read_gbq(query, dialect=\"standard\")\n",
        "# people with at least one icd9/10cm code in observation but no smoking code (including chewing tobacco)\n",
        "# so use df_smoke_controls_exclusion person_ids\n",
        "# deduplicate the person_ids\n",
        "query = (\"\"\"\n",
        "SELECT distinct person_id\n",
        "FROM\n",
        "    (SELECT DISTINCT person_id, observation_source_concept_id, observation_source_value, observation_date\n",
        "        FROM `\"\"\"+ str(CDR_version) +\"\"\".observation`) AS obs\n",
        "     INNER JOIN\n",
        "        (SELECT DISTINCT concept_id, concept_name, concept_code, vocabulary_id\n",
        "            FROM `\"\"\"+str(CDR_version)+\"\"\".concept`\n",
        "            where (vocabulary_id ='ICD9CM') or\n",
        "            (vocabulary_id ='ICD10CM')) as concept\n",
        "            on concept.concept_id = obs.observation_source_concept_id\n",
        "       where person_id not in \"\"\"+\"(\"+' '.join([str(id)+\",\" for id in np.unique(df_smoke_controls_exclusion.person_id)])[:-1]+\")\"+\"\"\"\n",
        "\"\"\")\n",
        "observation_coded_participants_no_smoke = pd.read_gbq(query, dialect=\"standard\")\n",
        "\n",
        "# append to smoke_nos from condition occurrence\n",
        "smoke_nos = smoke_nos.append(observation_coded_participants_no_smoke)\n",
        "# deduplicate\n",
        "smoke_nos = smoke_nos.drop_duplicates()\n",
        "smoke_nos[\"ever_smoke_ehr\"] = 0\n",
        "smoke_nos.to_csv(\"smoke_nos.csv\")\n",
        "# and remove folks with only one code for smoke\n",
        "df_smoke_ehr = df_smoke_cases_inclusion[['person_id','ever_smoke_ehr']]\n",
        "\n",
        "df_smoke_ehr = df_smoke_ehr.append(smoke_nos)\n",
        "df_smoke_ehr.head() # this is what we will use in our analysis\n",
        "df_smoke_ehr.to_csv(\"df_smoke_ehr.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V1o9Sn-zG3z"
      },
      "outputs": [],
      "source": [
        "df_smoke_ehr.ever_smoke_ehr.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2v2Xj6SSzG3z"
      },
      "source": [
        "__Extract covariats for participants with EHR smoking status__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIuY38-HzG30"
      },
      "outputs": [],
      "source": [
        "# subset out people with 1 code in covariates, recall that they were coded as -9\n",
        "demo_patients_cov = make_covariates(df_smoke_ehr[df_smoke_ehr['ever_smoke_ehr']!=-9], CDR_version = CDR_version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Kbv3jFzG31"
      },
      "source": [
        "__Extract participants who have any EHR data for inclusion in the analysis__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "itmzn68JzG31"
      },
      "outputs": [],
      "source": [
        "# now cross-reference with ehr folks\n",
        "\n",
        "query = \"\"\"\n",
        "WITH ehr AS (\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.measurement` as m\n",
        "left join `{dataset}.measurement_ext` as mm on m.measurement_id = mm.measurement_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.condition_occurrence` as m\n",
        "left join `{dataset}.condition_occurrence_ext` as mm on m.condition_occurrence_id = mm.condition_occurrence_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.device_exposure` as m\n",
        "left join `{dataset}.device_exposure_ext` as mm on m.device_exposure_id = mm.device_exposure_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.drug_exposure` as m\n",
        "left join `{dataset}.drug_exposure_ext` as mm on m.drug_exposure_id = mm.drug_exposure_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.observation` as m\n",
        "left join `{dataset}.observation_ext` as mm on m.observation_id = mm.observation_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.procedure_occurrence` as m\n",
        "left join `{dataset}.procedure_occurrence_ext` as mm on m.procedure_occurrence_id = mm.procedure_occurrence_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        "\n",
        "union distinct\n",
        "\n",
        "Select\n",
        "   distinct person_id\n",
        "from `{dataset}.visit_occurrence` as m\n",
        "left join `{dataset}.visit_occurrence_ext` as mm on m.visit_occurrence_id = mm.visit_occurrence_id\n",
        "where lower(mm.src_id) like 'ehr site%'\n",
        ")\n",
        "\n",
        "select distinct\n",
        "\"EHR\" as data,\n",
        "person_id\n",
        "FROM ehr\n",
        "\"\"\"\n",
        "\n",
        "ehr_df = pd.read_gbq(query.format(dataset=CDR_version), dialect='standard')\n",
        "ehr_df.to_csv(\"ehr_df.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqjXTe7AzG31"
      },
      "source": [
        "__Run the following cell only if you want to read the above files after saving them__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ2yeXY2zG32"
      },
      "outputs": [],
      "source": [
        "df_smoke_ehr=pd.read_csv(\"df_smoke_ehr.csv\")\n",
        "df_smoke_cases_inclusion=pd.read_csv(\"df_smoke_cases_inclusion.csv\")\n",
        "df_smoke_cases_inclusion=pd.read_csv(\"df_smoke_controls_exclusion.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XgjQ1J2zG32"
      },
      "source": [
        "__Extracting infomration about smoking from surveys__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Om1u2YpzG33"
      },
      "outputs": [],
      "source": [
        "## Check out w.ppi smoking\n",
        "# inner join the question value_as_concept_id to the concept_id of the response\n",
        "#1585857\n",
        "query =(\"\"\"\n",
        "select distinct qs.person_id, conc.concept_name from\n",
        "(SELECT DISTINCT person_id, observation_source_value, observation_source_concept_id, value_source_concept_id\n",
        "    FROM `\"\"\"+ str(CDR_version)+\"\"\".observation` where observation_source_concept_id =1585857) as qs\n",
        "INNER JOIN\n",
        "(SELECT concept_id, concept_name from `\"\"\"+ str(CDR_version)+\"\"\".concept`) as conc\n",
        "on conc.concept_id = qs.value_source_concept_id\n",
        "\"\"\")\n",
        "obs_q = pd.read_gbq(query, dialect=\"standard\")\n",
        "obs_q.concept_name.value_counts()\n",
        "\n",
        "\n",
        "## We now exract the person id and concept name (responses)\n",
        "PPI_ever_smoke = obs_q[[\"person_id\", \"concept_name\"]]\n",
        "\n",
        "# Subset to where we have smoking info by merging with PPI smoke\n",
        "# We just want people who responded either yes or no - since skip/prefer not to answer is not very informative\n",
        "PPI_smoke_Y_N = PPI_ever_smoke[(PPI_ever_smoke[\"concept_name\"]==\"100 Cigs Lifetime: No\")|(PPI_ever_smoke[\"concept_name\"]==\"100 Cigs Lifetime: Yes\")]\n",
        "\n",
        "PPI_smoke_Y_N[\"ever_smoke_ppi\"] = (PPI_smoke_Y_N[\"concept_name\"]==\"100 Cigs Lifetime: Yes\")+0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jk7GnHXDzG33"
      },
      "outputs": [],
      "source": [
        "demo_patients_cov_ppi = make_covariates(PPI_smoke_Y_N[[\"person_id\",\"ever_smoke_ppi\"]], CDR_version = CDR_version)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTZqXExKzG4C"
      },
      "outputs": [],
      "source": [
        "demo_patients_cov_ppi.to_csv(\"demo_patients_cov_ppi.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkWSna73zG4C"
      },
      "outputs": [],
      "source": [
        "PPI_ever_smoke.groupby(\"concept_name\",as_index=False).count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptZz5jJ4zG4D"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGRZArbPzG4D"
      },
      "source": [
        "__Reading the files that will be used in the PheWAS analysis__\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_6DCCT6zG4E"
      },
      "outputs": [],
      "source": [
        "phecodes_patients_counts=pd.read_csv(\"phecodes_patients_counts.csv\")\n",
        "demo_patients_cov=pd.read_csv(\"demo_patients_cov.csv\")\n",
        "demo_patients_cov_ppi=pd.read_csv(\"demo_patients_cov_ppi.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAfVWuUUzG4E"
      },
      "outputs": [],
      "source": [
        "demo_patients_cov.shape,demo_patients_cov_ppi.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zIDctnYnzG4F"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHf8HOeszG4F"
      },
      "outputs": [],
      "source": [
        "print(\"Number of smokers using EHR\",demo_patients_cov[\"ever_smoke_ehr\"].sum())\n",
        "print(\"Number of smokers using surveys\",demo_patients_cov_ppi[\"ever_smoke_ppi\"].sum())\n",
        "ehr_smokes=demo_patients_cov[demo_patients_cov[\"ever_smoke_ehr\"]==1].person_id.tolist()\n",
        "ppi_smokes=demo_patients_cov_ppi[demo_patients_cov_ppi[\"ever_smoke_ppi\"]==1].person_id.tolist()\n",
        "print(\"Overlap between EHR and survey\",len(set(ehr_smokes).intersection(ppi_smokes)))\n",
        "print(\"Smoker only in EHR\",len(set(ehr_smokes).difference(ppi_smokes)))\n",
        "print(\"Smoker only in Survey\",len(set( ppi_smokes).difference(ehr_smokes)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqUtUgLpzG4F"
      },
      "source": [
        "__Demographics for participants who will be inlcuded in Smkoing EHR analysis__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APlP6OSCzG4G"
      },
      "outputs": [],
      "source": [
        "print(\"Demographics of participants who will be inlcuded in Smkoing EHR analysis\")\n",
        "demo_patients_cov.shape[0],demo_patients_cov[\"white\"].sum(),demo_patients_cov[\"AF\"].sum(),demo_patients_cov[\"Asian\"].sum(),demo_patients_cov[\"race_unk\"].sum(),demo_patients_cov[\"Other\"].sum()\n",
        "demo_ehr_race=pd.DataFrame(demo_patients_cov[[\"white\",\"AF\",\"Asian\",\"race_unk\",\"Other\"]].sum()).reset_index()\n",
        "demo_ehr_race.columns=[\"Race\",\"Count\"]\n",
        "demo_ehr_race[\"Perc\"]=100*demo_ehr_race[\"Count\"]/demo_patients_cov.shape[0]\n",
        "demo_ehr_race"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ngQ2V9yzG4H"
      },
      "outputs": [],
      "source": [
        "\n",
        "print(\"Sex at Birth of participants who will be inlcuded in Smkoing EHR analysis\")\n",
        "demo_ehr_sex=pd.DataFrame(demo_patients_cov[['female', 'male', 'unk_sex']].sum()).reset_index()\n",
        "demo_ehr_sex.columns=[\"Race\",\"Count\"]\n",
        "demo_ehr_sex[\"Perc\"]=100*demo_ehr_sex[\"Count\"]/demo_patients_cov.shape[0]\n",
        "demo_ehr_sex\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HIIUiRMzG4I"
      },
      "source": [
        "__Demographics for participants who will be inlcuded in Surevy Smkoing analysis__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktykcOpEzG4J"
      },
      "outputs": [],
      "source": [
        "print(\"Race of participants who will be inlcuded in Smkoing survey analysis\")\n",
        "demo_ppi_race=pd.DataFrame(demo_patients_cov_ppi[[\"white\",\"AF\",\"Asian\",\"race_unk\",\"Other\"]].sum()).reset_index()\n",
        "demo_ppi_race.columns=[\"Race\",\"Count\"]\n",
        "demo_ppi_race[\"Perc\"]=100*demo_ppi_race[\"Count\"]/demo_patients_cov_ppi.shape[0]\n",
        "demo_ppi_race\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqtft2XUzG4J"
      },
      "outputs": [],
      "source": [
        "print(\"Sex at birth of participants who will be inlcuded in Smkoing survey analysis\")\n",
        "demo_ppi_sex=pd.DataFrame(demo_patients_cov_ppi[['female', 'male', 'unk_sex']].sum()).reset_index()\n",
        "demo_ppi_sex.columns=[\"Race\",\"Count\"]\n",
        "demo_ppi_sex[\"Perc\"]=100*demo_ppi_sex[\"Count\"]/demo_patients_cov_ppi.shape[0]\n",
        "demo_ppi_sex\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eV060jXzG4K"
      },
      "outputs": [],
      "source": [
        "print(\"Phecode groups\")\n",
        "phecode_info[\"group\"].unique().tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Bhhqi5EzG4K"
      },
      "source": [
        "__Running analysis on respiratory phecodes__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PoQVIDNzG4K"
      },
      "source": [
        "__EHR smoking PheWAS analysis__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dY578NWXzG4L"
      },
      "outputs": [],
      "source": [
        "respiratory=phecode_info[phecode_info[\"group\"]==\"respiratory\"][\"phecode\"].unique().tolist()\n",
        "# Instantiate an object of class PheWAS\n",
        "test_pool = PheWAS_Pool(phecode_counts = phecodes_patients_counts,\n",
        "              covariates= demo_patients_cov, CDR_version= CDR_version,\n",
        "             independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                    \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "            genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\n",
        "                                              \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "              indep_var_of_interest= \"ever_smoke_ehr\",show_res = True,\n",
        "            phecode_process= np.array(respiratory),\n",
        "              cores =16,\n",
        "            #independent_var_names=[\"male\", \"ever_smoke_ehr\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2fZpXpQfzG4L"
      },
      "outputs": [],
      "source": [
        "test_pool.run()\n",
        "test_pool.logit_Phecode_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3HJ0g89zG4M"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This snippet assumes you run setup first\n",
        "\n",
        "# This code saves your PheWAS results into a csv file in a \"data\" folder in Google Bucket\n",
        "\n",
        "# Replace df with THE NAME OF YOUR DATAFRAME\n",
        "my_dataframe = test_pool.logit_Phecode_results\n",
        "\n",
        "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
        "destination_filename = 'phewas_smoking_ehr_9_2023_demo_respiratory_project.csv'\n",
        "\n",
        "########################################################################\n",
        "##\n",
        "################# DON'T CHANGE FROM HERE ###############################\n",
        "##\n",
        "########################################################################\n",
        "\n",
        "# save dataframe in a csv file in the same workspace as the notebook\n",
        "my_dataframe.to_csv(destination_filename, index=False)\n",
        "\n",
        "# get the bucket name\n",
        "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
        "\n",
        "# copy csv file to the bucket\n",
        "os.system(f\"gsutil cp './{destination_filename}' '{my_bucket}/data/'\")\n",
        "print(f'[INFO] {destination_filename} is successfully uploaded in your bucket.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzZMkjLtzG4M"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNEv7mr-zG4M"
      },
      "source": [
        "__Survey smoking PheWAS analysis__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWdAjR68zG4M"
      },
      "source": [
        "__Running the PheWAS on two sections since the dataset will be big and your kernel might die__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TIueJ_4bzG4N"
      },
      "outputs": [],
      "source": [
        "respiratory=phecode_info[phecode_info[\"group\"]==\"respiratory\"][\"phecode\"].unique().tolist()[:40]\n",
        "# Instantiate an object of class PheWAS\n",
        "Phewas_part1 = PheWAS_Pool(phecode_counts = phecodes_patients_counts,\n",
        "              covariates= demo_patients_cov_ppi, CDR_version= CDR_version,\n",
        "             independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                    \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "            genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\n",
        "                                              \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "              indep_var_of_interest= \"ever_smoke_ppi\",show_res = True,\n",
        "            phecode_process= np.array(respiratory),\n",
        "              cores =16,\n",
        "            #independent_var_names=[\"male\", \"ever_smoke_ehr\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YsIEYckGzG4N"
      },
      "outputs": [],
      "source": [
        "Phewas_part1.run()\n",
        "Phewas_part1.logit_Phecode_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vexyM3sCzG4N"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This snippet assumes you run setup first\n",
        "\n",
        "# This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
        "\n",
        "# Replace df with THE NAME OF YOUR DATAFRAME\n",
        "my_dataframe = Phewas_part1.logit_Phecode_results\n",
        "\n",
        "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
        "destination_filename = 'phewas_smoking_ppi_9_2023_demo_respiratory_project_part2.csv'\n",
        "\n",
        "########################################################################\n",
        "##\n",
        "################# DON'T CHANGE FROM HERE ###############################\n",
        "##\n",
        "########################################################################\n",
        "\n",
        "# save dataframe in a csv file in the same workspace as the notebook\n",
        "my_dataframe.to_csv(destination_filename, index=False)\n",
        "\n",
        "# get the bucket name\n",
        "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
        "\n",
        "# copy csv file to the bucket\n",
        "os.system(f\"gsutil cp './{destination_filename}' '{my_bucket}/data/'\")\n",
        "print(f'[INFO] {destination_filename} is successfully uploaded in your bucket.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVZquqtXzG4O"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEpydXVmzG4O"
      },
      "outputs": [],
      "source": [
        "respiratory=phecode_info[phecode_info[\"group\"]==\"respiratory\"][\"phecode\"].unique().tolist()[40:]\n",
        "# Instantiate an object of class PheWAS\n",
        "Phewas_part2 = PheWAS_Pool(phecode_counts = phecodes_patients_counts,\n",
        "              covariates= demo_patients_cov_ppi, CDR_version= CDR_version,\n",
        "             independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                    \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "            genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\n",
        "                                              \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "              indep_var_of_interest= \"ever_smoke_ppi\",show_res = True,\n",
        "            phecode_process= np.array(respiratory),\n",
        "              cores =16,\n",
        "            #independent_var_names=[\"male\", \"ever_smoke_ehr\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mtj_Js_YzG4O"
      },
      "outputs": [],
      "source": [
        "Phewas_part2.run()\n",
        "Phewas_part2.logit_Phecode_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CFI7vqYYzG4P"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# This snippet assumes you run setup first\n",
        "\n",
        "# This code saves your dataframe into a csv file in a \"data\" folder in Google Bucket\n",
        "\n",
        "# Replace df with THE NAME OF YOUR DATAFRAME\n",
        "my_dataframe = Phewas_part2.logit_Phecode_results\n",
        "\n",
        "# Replace 'test.csv' with THE NAME of the file you're going to store in the bucket (don't delete the quotation marks)\n",
        "destination_filename = 'phewas_smoking_ppi_9_2023_demo_respiratory_project_part2.csv'\n",
        "\n",
        "########################################################################\n",
        "##\n",
        "################# DON'T CHANGE FROM HERE ###############################\n",
        "##\n",
        "########################################################################\n",
        "\n",
        "# save dataframe in a csv file in the same workspace as the notebook\n",
        "my_dataframe.to_csv(destination_filename, index=False)\n",
        "\n",
        "# get the bucket name\n",
        "my_bucket = os.getenv('WORKSPACE_BUCKET')\n",
        "\n",
        "# copy csv file to the bucket\n",
        "os.system(f\"gsutil cp './{destination_filename}' '{my_bucket}/data/'\")\n",
        "print(f'[INFO] {destination_filename} is successfully uploaded in your bucket.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9K1CjXozG4P"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ZS0KIgDzG4P"
      },
      "source": [
        "__You can read the files later for analysis without needing to run the code__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WDPd1BlazG4Q"
      },
      "outputs": [],
      "source": [
        "#plot manhaten plot\n",
        "phewas_smoking_ehr_9_2023_demo_respiratory_project=pd.read_csv(\"phewas_smoking_ehr_9_2023_demo_respiratory_project.csv\")\n",
        "phewas_smoking_ehr_9_2023_demo_respiratory_project\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "53fxxHqEzG4Q"
      },
      "outputs": [],
      "source": [
        "phewas_smoking_ehr_9_2023_demo_respiratory_project.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHFW60LMzG4Q"
      },
      "outputs": [],
      "source": [
        "respiratory=phecode_info[phecode_info[\"group\"]==\"respiratory\"][\"phecode\"].unique().tolist()[40:]\n",
        "# Instantiate an object of class PheWAS\n",
        "test_pool = PheWAS_Pool(phecode_counts = phecodes_patients_counts,\n",
        "              covariates= demo_patients_cov_ppi, CDR_version= CDR_version,\n",
        "             independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                    \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "            genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\n",
        "                                              \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "              indep_var_of_interest= \"ever_smoke_ppi\",show_res = True,\n",
        "            phecode_process= np.array(respiratory),\n",
        "              cores =16,\n",
        "            #independent_var_names=[\"male\", \"ever_smoke_ehr\"]\n",
        "        )\n",
        "test_pool.logit_Phecode_results=phewas_smoking_ehr_9_2023_demo_respiratory_project\n",
        "test_pool.Manhattan_Plot_Plus()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvEvv4r7zG4Q"
      },
      "outputs": [],
      "source": [
        "phewas_smoking_ppi_9_2023_demo_respiratory_project_part1=pd.read_csv(\"phewas_smoking_ppi_9_2023_demo_respiratory_project_part1.csv\")\n",
        "phewas_smoking_ppi_9_2023_demo_respiratory_project_part2=pd.read_csv(\"phewas_smoking_ppi_9_2023_demo_respiratory_project_part2.csv\")\n",
        "phewas_smoking_ppi_9_2023_demo_respiratory_project=pd.concat([phewas_smoking_ppi_9_2023_demo_respiratory_project_part1,phewas_smoking_ppi_9_2023_demo_respiratory_project_part2])\n",
        "\n",
        "respiratory=phecode_info[phecode_info[\"group\"]==\"respiratory\"][\"phecode\"].unique().tolist()[40:]\n",
        "# Instantiate an object of class PheWAS\n",
        "test_pool = PheWAS_Pool(phecode_counts = phecodes_patients_counts,\n",
        "              covariates= demo_patients_cov_ppi, CDR_version= CDR_version,\n",
        "             independent_var_names=[\"AF\",\"white\",\"Asian\",\"male\",\"age_at_last_event\",\n",
        "                                    \"ehr_length\",\"code_cnt\", \"unk_sex\", \"race_unk\", \"hisp_lat\",\"unk_eth\"],\n",
        "            genderspec_independent_var_names=[\"AF\",\"white\",\"Asian\",\"age_at_last_event\",\n",
        "                                              \"ehr_length\",\"code_cnt\",\"race_unk\",\"hisp_lat\",\"unk_eth\"],\n",
        "              indep_var_of_interest= \"ever_smoke_ppi\",show_res = True,\n",
        "            phecode_process= np.array(respiratory),\n",
        "              cores =16,\n",
        "            #independent_var_names=[\"male\", \"ever_smoke_ehr\"]\n",
        "        )\n",
        "test_pool.logit_Phecode_results=phewas_smoking_ppi_9_2023_demo_respiratory_project\n",
        "test_pool.Manhattan_Plot_Plus()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NipgtfbVzG4R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaioaPS2zG4R"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyXzg7GBzG4R"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}